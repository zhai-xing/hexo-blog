{
    "version": "https://jsonfeed.org/version/1",
    "title": "Mo Bai • All posts by \"原理\" tag",
    "description": "欢迎来到墨白的知识小屋，这里你可以阅读我的学习笔记并提出独到的见解~我们将互相学习交流知识，共同进步",
    "home_page_url": "https://zhai-xing.github.io/hexo-blog",
    "items": [
        {
            "id": "https://zhai-xing.github.io/hexo-blog/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0",
            "url": "https://zhai-xing.github.io/hexo-blog/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0",
            "title": "Redis学习笔记",
            "date_published": "2023-09-05T14:57:10.000Z",
            "content_html": "<h1 id=\"一-redis数据类型篇\"><a class=\"markdownIt-Anchor\" href=\"#一-redis数据类型篇\">#</a> 一、redis 数据类型篇</h1>\n<p>rdis 常见的数据类型及应用场景</p>\n<h4 id=\"string\"><a class=\"markdownIt-Anchor\" href=\"#string\">#</a> String</h4>\n<p>String 是最基本的 key-value 结构，key 是唯一标识 value 是具体的值 value 不仅是字符串，还可以是数字 (整数或者浮点数) value 最多可以容纳的数据长度是 512M，</p>\n<h5 id=\"内部实现\"><a class=\"markdownIt-Anchor\" href=\"#内部实现\">#</a> 内部实现：</h5>\n<p>String 底层实现的数据结构是 int 和 SDS (简单动态字符串)</p>\n<ul>\n<li>SDS 不仅可以保存文本数据还可以保存二进制数据，SDS 使用了 len 属性来判断字符串是否结束，</li>\n<li>SDS 获取字符串长度的时间复杂度是 O1</li>\n<li>Redis 的 SDS api 是安全的，拼接字符前会判断空间是否满足要求，不满足会自动扩容，所以不好导致缓冲区溢出</li>\n</ul>\n<h5 id=\"常用场景\"><a class=\"markdownIt-Anchor\" href=\"#常用场景\">#</a> 常用场景：</h5>\n<ul>\n<li>常规计数：计算点赞、转发、库存数量、阅读量</li>\n<li>分布式： 使用命令不存在此键就插入成功，而解锁 就是删除键，解锁的额操作需要判断，使用需要保证原子性操作，可以使用 Lua 脚本</li>\n<li>共享 Session 使用 Session 来保存用户的会话状态。</li>\n<li>热点数据缓存</li>\n</ul>\n<h4 id=\"list\"><a class=\"markdownIt-Anchor\" href=\"#list\">#</a> List</h4>\n<p>list 是简单的字符串列表，按照插入顺序排序，可以从头部或尾部向 List 添加元素，列最大长度为 2^32-1</p>\n<h5 id=\"内部实现-2\"><a class=\"markdownIt-Anchor\" href=\"#内部实现-2\">#</a> 内部实现</h5>\n<p>在 3.2 版本之前内部是采用双向链表或者压缩列表实现的，在后面就只用 quicklist 实现，替代了双向链表和压缩列表</p>\n<h5 id=\"常用场景-2\"><a class=\"markdownIt-Anchor\" href=\"#常用场景-2\">#</a> 常用场景</h5>\n<ul>\n<li>消息队列 ：如果要实现消息队列，需要实现消息的保序、可靠、处理重复的消息<br>\n保序的话，List 本身就是先进先出，已经是有序的了，并且 redis 提供了 BRPOP 命令，称为阻塞式读取，客户端在没有读到 redis 数据时自动阻塞，直到有数据了在读取。 处理重复消息，需要自己生成一个全局 ID，需要记录已经处理过的消息 ID. 而在消息可靠性方面，redis 在用户读取消息后就不会保存，若消费者消费失败消息就丢失了， 对于这个问题，可以再开一个消息队列，作为备份暂存，消费成功后再去删除掉备份的即可。<br>\n存在的问题</li>\n</ul>\n<ol>\n<li>无法支持消费者组</li>\n<li>无法支持多个消费者消费同一个消息</li>\n</ol>\n<h4 id=\"hash\"><a class=\"markdownIt-Anchor\" href=\"#hash\">#</a> Hash</h4>\n<p>Hash 是一个键值对集合，其中 value=[{field1,value1},{fieldN,valueN}]</p>\n<h5 id=\"内部实现-3\"><a class=\"markdownIt-Anchor\" href=\"#内部实现-3\">#</a> 内部实现</h5>\n<p>hash 类型的底层数据结构采用的是压缩列表或哈希表 。如果 hash 类型的元素格式小于 512 个 并且值小于 64 字节， 就使用压缩列表，反之则使用 hash 表 而在 redis7.0 中，压缩列表数据结构被废弃了，就采用 listpack 来实现</p>\n<h5 id=\"使用场景\"><a class=\"markdownIt-Anchor\" href=\"#使用场景\">#</a> 使用场景</h5>\n<p>通常用来缓存一些对象的属性，例如用户信息、购物车（用户 Id，商品 id，数量）</p>\n<h4 id=\"set\"><a class=\"markdownIt-Anchor\" href=\"#set\">#</a> Set</h4>\n<p>set 类型是无序唯一的键值集合，他的存储顺序不会按照插入的先后来存储，一个集合最多可存储 2^32-1 个元素，可以进行并交差集运算，也可以支持多个集合去交集、并集、差集。</p>\n<h5 id=\"应用场景\"><a class=\"markdownIt-Anchor\" href=\"#应用场景\">#</a> 应用场景</h5>\n<p>Set 类型比较适合用来做数据去重和保障数据的唯一性，还可以用来统计多个集合的交集、并集、补集，当我们存储的数据是无序且需要去重的情况下，比较适合使用集合类型来存储。需要注意 set 的集合计算复杂度较高，在数据量大的情况下，直接执行这些计算会导致 Redis 实例阻塞，</p>\n<ul>\n<li>点赞记录：一个用户只能对一篇文章点赞</li>\n<li>共同关注 ：交集</li>\n<li>抽奖活动：防止重复中奖</li>\n</ul>\n<h4 id=\"zset\"><a class=\"markdownIt-Anchor\" href=\"#zset\">#</a> Zset</h4>\n<p>zset 相比较与 set 类型多了一个排序属性，score 分值。对于有序集合 zset 每个存储元素相当于是有两个值组成，一个是有序集合的元素值，一个是排序值，</p>\n<h5 id=\"内部实现-4\"><a class=\"markdownIt-Anchor\" href=\"#内部实现-4\">#</a> 内部实现</h5>\n<p>内部采用了压缩列表或跳表实现的，若有有序集合元素个数小于 128 个。并且每个元素值小于 64 字节。redis 会使用压缩列表，否则则使用跳表。在 redis7.0 中跳表废弃了使用了 listpack 数据结构来实现</p>\n<h5 id=\"应用场景-2\"><a class=\"markdownIt-Anchor\" href=\"#应用场景-2\">#</a> 应用场景</h5>\n<p>排行榜、电话姓名、有序排列</p>\n<hr>\n<p>高级数据类型</p>\n<h4 id=\"bitmap\"><a class=\"markdownIt-Anchor\" href=\"#bitmap\">#</a> BitMap</h4>\n<p>位图，是一串连续的二进制数组 [0,1] 可以通过 offset 定位元素，BitMap 通过最小的单位 bit 来进行 0|1 的设置，表示某个元素的值或状态，时间复杂度为 O1,</p>\n<p>内部实现：本身利用了 String 作为底层数据结构，String 会保存为二进制的字节数组，redis 就把每个 bit 位利用起来，用来表示一个元素的二进制状态。</p>\n<p>应用场景：<br>\n签到打卡，判断用户登录状态 连续前端用户数，</p>\n<h4 id=\"hyperloglog\"><a class=\"markdownIt-Anchor\" href=\"#hyperloglog\">#</a> HyperLogLog</h4>\n<p>是一种用于统计基数的数据集合类型，基数统计是指统计一个集合中不重复元素个数， HyperLoglog 的统计规则是基于概率完成的，不是非常准确，而 HyperLogLog 的优点在于，输入元素的数量或体积很大时，计算基数所需要的内存空间是固定且很小的。<br>\n应用场景： 百万级 UV 网页计数</p>\n<h4 id=\"geo\"><a class=\"markdownIt-Anchor\" href=\"#geo\">#</a> GEO</h4>\n<p>这个是用于存储地理位置信息的，并可以对存储的信息进行计算操作，例如搜索附近的餐馆，打车等等。<br>\n内部原理：<br>\n底层采用了 Sorted Set 集合类型，GEO 类型使用了 GOEhash 编码方法实现了经纬度到 sorted set 中元素权重分数的转换，其中的两个关键机制计算对二维地图做区间划分和对区间进行编码。一组经纬度落在某个区间后，就用区间的编码值来标识。</p>\n<h4 id=\"stream\"><a class=\"markdownIt-Anchor\" href=\"#stream\">#</a> Stream</h4>\n<p>redis5. 新增的消息队列数据类型，用于完美地实现消息队列，它支持消息的持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式等，让消息队列更加的稳定和可靠</p>\n<ul>\n<li>消息保序：XADD/XREAD</li>\n<li>阻塞读取：XREAD block</li>\n<li>重复消息处理：Stream 在使用 XADD 命令，会自动生成全局唯一 ID；</li>\n<li>消息可靠性：内部使用 PENDING List 自动保存消息，使用 XPENDING 命令查看消费组已经读取但是未被确认的消息，消费者使用 XACK 确认消息；</li>\n<li>支持消费组形式消费数据</li>\n</ul>\n<h1 id=\"二-redis数据结构篇\"><a class=\"markdownIt-Anchor\" href=\"#二-redis数据结构篇\">#</a> 二、Redis 数据结构篇</h1>\n<p>redis 本身就是一个键值型的数据结构<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225928934.png\" alt=\"image-20230905225928934\"></p>\n<ul>\n<li>redisDb 结构，表示 Redis 数据库的结构，结构体里存放了指向 dict 结构的指针。</li>\n<li>dict 结构，结构体里存放了 2 个哈希表，正常情况下都是用哈希表 1，哈希表 2 再 rehash 的时候才会使用</li>\n<li>dictht 结构表示哈希表的结构，结构体存放了哈希表数组，每个数组都指向应该哈希表节点的结构体指针 dictEntry 结构，表示哈希表节点的结构，结构里存放了 **void * key 和 void * value 指针， key 指向的是 String 对象，而 value 则可以指向 String 对象，也可以指向集合类型的对象，比如 List 对象、Hash 对象、Set 对象和 Zset 对象。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225939660.png\" alt=\"image-20230905225939660\"></li>\n</ul>\n<h3 id=\"sds\"><a class=\"markdownIt-Anchor\" href=\"#sds\">#</a> SDS</h3>\n<p>redis 是使用 C 语言实现的，但是他没有直接使用 C 语言的 char* 字符数组，而是自己封装了一个名为简单动态字符串的数据结构，来表示字符串， 也就是 SDS<br>\n 不使用 c 语言的默认字符数组是因为:</p>\n<ol>\n<li>C 语言默认的字符数组是以 \\0 表示结束的，在二进制数据中经常有 \\0 这样的数据串，使用就不能保存</li>\n<li>C 语言的字符串是不会记录自身的缓冲区大小的。容易发生溢出</li>\n<li>字符串操作函数不高效且不安全，比如有缓冲区溢出的风险，有可能会造成程序运行终止<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225948684.png\" alt=\"image-20230905225948684\"></li>\n</ol>\n<ul>\n<li>SDS 的自动扩容机制 如果所需要的长度小于 1mb 。那么是翻倍扩容，如果超过 1Mb 是按照 newlen=1mb</li>\n<li>flags，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，这 5 种类型的主要区别就在于，它们数据结构中的 len 和 alloc 成员变量的数据类型不同。\n<ul>\n<li>sdshdr16 类型的 len 和 alloc 的数据类型都是 uint16_t，表示字符数组长度和分配空间大小不能超过 2 的 16 次方。</li>\n<li>sdshdr32 则都是 uint32_t，表示表示字符数组长度和分配空间大小不能超过 2 的 32 次方。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"链表\"><a class=\"markdownIt-Anchor\" href=\"#链表\">#</a> 链表</h3>\n<p>redis 的链表结构很简单，就前置节点，后置节点，数据；但是封装了一个 List 数据结构</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">typedef struct list &#123;</span><br><span class=\"line\">    //链表头节点</span><br><span class=\"line\">    listNode *head;</span><br><span class=\"line\">    //链表尾节点</span><br><span class=\"line\">    listNode *tail;</span><br><span class=\"line\">    //节点值复制函数</span><br><span class=\"line\">    void *(*dup)(void *ptr);</span><br><span class=\"line\">    //节点值释放函数</span><br><span class=\"line\">    void (*free)(void *ptr);</span><br><span class=\"line\">    //节点值比较函数</span><br><span class=\"line\">    int (*match)(void *ptr, void *key);</span><br><span class=\"line\">    //链表节点数量</span><br><span class=\"line\">    unsigned long len;</span><br><span class=\"line\">&#125; list;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230013442.png\" alt=\"image-20230905230013442\"></p>\n<ul>\n<li>ListNode 链表节点的结构里设置有 prev 和 next，获取某个节点的前置节点或后置节点的时间复杂度只需要 O (1)；</li>\n<li>listl 因为有表头指针和标为指针，所以获取表头和表尾节点的时间复杂度是 O (1)</li>\n<li>list 结构因为提供了链表节点数量 len，所以获取链表中的节点数量的时间复杂度只需 O (1)；</li>\n</ul>\n<h3 id=\"压缩列表\"><a class=\"markdownIt-Anchor\" href=\"#压缩列表\">#</a> 压缩列表</h3>\n<p>压缩列表的最大特点就是他被设计成一种内存紧凑型的数据结构，占用的是一块连续的内存空间，不仅可以利用 CPU 缓存，而且可以针对不同的长度的数据进行相应编码，这种方式可以有效的节省内存开销。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230025692.png\" alt=\"image-20230905230025692\"></p>\n<ul>\n<li>zlbytes 记录整个压缩列表占用内存对内存字节数</li>\n<li>zltail 记录压缩列表尾部节点距离起始地址由多少字节，也就是列尾偏移量，</li>\n<li>zllen：记录压缩列表包含的节点数量</li>\n<li>zlend: 标记压缩列表的结束点 1</li>\n<li>压缩列表查找表头和表尾元素很快，只需要 O (1) 但是查找其他元素就没那么快了，因此压缩列表不适合保存过多元素</li>\n</ul>\n<h3 id=\"哈希表\"><a class=\"markdownIt-Anchor\" href=\"#哈希表\">#</a> 哈希表</h3>\n<p>哈希表是一种保存键值对（key-value）的数据结构。<br>\n哈希表中的每一个 key 都是独一无二的，程序可以根据 key 查找到与之关联的 value，或者通过 key 来更新 value，又或者根据 key 来删除整个 key-value 等等。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230038715.png\" alt=\"image-20230905230038715\"></p>\n<ul>\n<li>redis 采用了链式哈希的方式来解决冲突，</li>\n<li>不过，链式哈希局限性也很明显，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O (n)。</li>\n</ul>\n<p>随着链表越来越长，hash 的查找速度也就会降低，redis 这里提供了 rehash 也就是上面提到的。</p>\n<h4 id=\"rehash\"><a class=\"markdownIt-Anchor\" href=\"#rehash\">#</a> rehash</h4>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230048637.png\" alt=\"image-20230905230048637\"><br>\n 其实整个备份就是来做数据迁移了，节点太多 hash 桶太少，需要扩容<br>\n - 随着数据逐步增多，触发了 rehash 操作，这个过程分为三步：</p>\n<ul>\n<li>给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；</li>\n<li>将「哈希表 1 」的数据迁移到「哈希表 2」 中；</li>\n<li>迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备</li>\n<li>为了避免在 rehash 在数据迁移是，因为拷贝数据导致 redis 性能下降，所以都是采用的渐进式 hash，迁移工作是分多次完成，</li>\n</ul>\n<p>触发 rehash 时机：<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230059484.png\" alt=\"image-20230905230059484\"></p>\n<ul>\n<li>当负载因子大于等于 1 ，并且 Redis 没有在执行 bgsave 命令或者 bgrewiteaof 命令，也就是没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。</li>\n<li>当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作</li>\n</ul>\n<h3 id=\"整数集合\"><a class=\"markdownIt-Anchor\" href=\"#整数集合\">#</a> 整数集合</h3>\n<p>整数集合是 Set 对象的底层实现之一，当一个 Set 对象只包含整数值元素，并且元素数量不大时，就用整数集这个数据结构作为底层实现之一，整数集合本质上是一块连续的内存空间吗，整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性</p>\n<h3 id=\"跳表\"><a class=\"markdownIt-Anchor\" href=\"#跳表\">#</a> 跳表</h3>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230108527.png\" alt=\"image-20230905230108527\"><br>\n 链表在查找元素的时候，因为需要逐一查找，所以查找效率非常低下，时间复杂度是 O (n) ，跳表是链表的改进版<br>\n，多层有序链表，redis 中只有 Zset 用到了跳表，，个人感觉应该是基于链表的二分查找，redis 为什么使用跳表，而不使用红黑树来实现有序集合。</p>\n<ol>\n<li>有序集合主要是有增、删、改、查四个操作，这些操作红黑树和跳表时间复杂度都是一样的</li>\n<li>但是基于区间的查询，红黑树的效率就太低了，所以使用跳表</li>\n</ol>\n<h3 id=\"quicklist\"><a class=\"markdownIt-Anchor\" href=\"#quicklist\">#</a> quickList</h3>\n<p>quicklist 其实是双向链表和压缩列表的组合，一个 quicklist 就是一个链表，而链表中每个元素又是一个压缩列表，<br>\n压缩列表的不足，如果保存的元素太多，或者元素变大，压缩列表会有连锁更新的情况，quicklist 解决办法，通过控制每个链表节点中的压缩列表的大小或者元素个数，来规避连锁更新的问题。因为压缩列表元素越少或越小，连锁更新带来的影响就越小，从而提供了更好的访问性能。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230119103.png\" alt=\"image-20230905230119103\"></p>\n<h3 id=\"listpack\"><a class=\"markdownIt-Anchor\" href=\"#listpack\">#</a> listpack</h3>\n<p>是为了解决压缩列表出现的连锁更新问题，目的是替代压缩列表，它最大特点是 listpack 中每个节点不再包含前一个节点的长度了，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230130155.png\" alt=\"image-20230905230130155\"><br>\nlistpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题。</p>\n<h1 id=\"三-reids-持久化\"><a class=\"markdownIt-Anchor\" href=\"#三-reids-持久化\">#</a> 三、Reids 持久化</h1>\n<h2 id=\"aof持久化\"><a class=\"markdownIt-Anchor\" href=\"#aof持久化\">#</a> AOF 持久化</h2>\n<p>redis 每执行一条写操作，就把该命令，以追加的方式写入到一个文件，然后后重启 redis 时，先去读这个这个文件里的命令并执行<br>\n配置文件中开启</p>\n <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">appendonly yes</span><br><span class=\"line\">appendfilename &quot;appendonly.aof&quot;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230142284.png\" alt=\"image-20230905230142284\"><br>\n 写入数据到数据库和写 aof 日志都是在主进程中完成的，有一定性能损失。当然 redis 也提供了其他的写回机制，可以配置，在 redis .conf 中配置 appendfsync</p>\n<ul>\n<li>Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；</li>\n<li>Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；</li>\n<li>No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230149806.png\" alt=\"image-20230905230149806\"></li>\n</ul>\n<h4 id=\"aof重写机制\"><a class=\"markdownIt-Anchor\" href=\"#aof重写机制\">#</a> AOF 重写机制</h4>\n<p>AOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。<br>\nredis 的重写机制是在后方子进程 bgrewriteaof 来完成的，<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230157631.png\" alt=\"image-20230905230157631\"></p>\n<h2 id=\"rdb持久化\"><a class=\"markdownIt-Anchor\" href=\"#rdb持久化\">#</a> RDB 持久化</h2>\n<p>RDB 是内存快照，就是记录一个瞬间的东西，记录的是实时数据，与 AOF 不同，AOF 记录的是命令操作日志，而不是实际的数据。在回复数据时，RDB 要快一些，只需要将 RDB 文件读入内存就可以了，不需要像 AOF 一样还需要执行额外的操作命令。</p>\n<h3 id=\"如何生成rdb\"><a class=\"markdownIt-Anchor\" href=\"#如何生成rdb\">#</a> 如何生成 RDB</h3>\n<p>redis 提供了两个命令，分别是 save 和 bgsave，执行了 save 命令会在主线程上生成 rdb 文件，如果写入 rdb 文件太多会阻塞主线程。执行 bgsave 是创建了一个进程来生成 rdb 文件，这样可以避免主线程阻塞。<br>\n也可以通过配置文件的选项，每隔一段时间自动执行 bgsave 命令，因为 RDB 快照是全量快照的方式，因此执行的频率不能太频繁，否则会影响 Redis 性能，</p>\n<ul>\n<li>执行快照是 redis 的数据是可以继续呗修改的，因为采用了写时复制技术，<br>\n执行 bgsave 命令的时候，会通过 fork () 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建  子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个。共享的内存当另一部分被用户修改时，因为采用了写时复制，所以做复制功能的线程也会被同步。</li>\n</ul>\n<h2 id=\"混合持久化\"><a class=\"markdownIt-Anchor\" href=\"#混合持久化\">#</a> 混合持久化</h2>\n<p>尽管 RDB 比 AOF 的数据恢复速度快，但是快照的频率不好把握：</p>\n<p>如果频率太低，两次快照间一旦服务器发生宕机，就可能会比较多的数据丢失；<br>\n如果频率太高，频繁写入磁盘和创建子进程会带来额外的性能开销。<br>\n这是 redis4.0 提出来的，在配置文件中开启</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">aof-use-rdb-preamble yes</span><br></pre></td></tr></table></figure>\n<p>当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。</p>\n<h1 id=\"功能篇\"><a class=\"markdownIt-Anchor\" href=\"#功能篇\">#</a> 功能篇</h1>\n<h3 id=\"过期删除策略\"><a class=\"markdownIt-Anchor\" href=\"#过期删除策略\">#</a> 过期删除策略</h3>\n<h5 id=\"定时删除\"><a class=\"markdownIt-Anchor\" href=\"#定时删除\">#</a> 定时删除</h5>\n<p>在设置 key 的过期时间时，同时创建一个定时事件，当到达时，由事件处理器执行 key 的删除操作</p>\n<ul>\n<li>优点：内存可以被尽快地释放。定时删除对内存是最友好的。</li>\n<li>缺点：定时删除策略对 CPU 不友好，删除过期 key 可能会占用相当一部分 CPU 时间，CPU 紧张的情况下将 CPU 用于删除和当前任务无关的过期键上，会对服务器的响应时间和吞吐量造成影响。</li>\n</ul>\n<h5 id=\"惰性删除\"><a class=\"markdownIt-Anchor\" href=\"#惰性删除\">#</a> 惰性删除</h5>\n<p>不主动删除过期健，每次从数据库访问 key 时检查是否过期，过期则删除，</p>\n<ul>\n<li>\n<p>优点：只会使用很少的系统资源，对 CPU 最友好。</p>\n</li>\n<li>\n<p>缺点：如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放。惰性删除策略对内存不友好。</p>\n</li>\n</ul>\n<h5 id=\"定期删除\"><a class=\"markdownIt-Anchor\" href=\"#定期删除\">#</a> 定期删除</h5>\n<p>每隔段时间随机从数据库中取出一定数量的 key 进行检查，并删除其中过期的 key</p>\n<ul>\n<li>优点：限制删除操作执行的时长和频率来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。</li>\n<li>缺点：内存清理方面没有定时删除效果好，同时没有惰性删除使用的系统资源少。难以确定删除操作执行的时长和频率</li>\n</ul>\n<h5 id=\"定期删除惰性删除配合使用\"><a class=\"markdownIt-Anchor\" href=\"#定期删除惰性删除配合使用\">#</a> 定期删除 + 惰性删除配合使用</h5>\n<p>redis 选择的时惰性删除 + 定期删除，配合使用，<br>\nRedis 在访问或者修改 key 之前，都会调用 expireIfNeeded 函数对其进行检查，检查 key 是否过期：</p>\n<ul>\n<li>如果过期，则删除该 key，然后返回 null 客户端；</li>\n<li>如果没有过期，不做任何处理，然后返回正常的键值对给客户端</li>\n</ul>\n<p>从过期字典中随机抽取 20 个 key；检查这 20 个 key 是否过期，并删除已过期的 key；已过期 key 的数量占比随机抽取 key 的数量大于 25%，则继续重复步骤直到比重小于 25%。</p>\n<h3 id=\"redis-事务\"><a class=\"markdownIt-Anchor\" href=\"#redis-事务\">#</a> redis 事务</h3>\n<p>严格来说 redis 事务只是个批处理，有隔离性但是没有原子性<br>\n Multi：开启事务<br>\n Exec：执行<br>\n Discard: 不执行<br>\n redis 和 lus 脚本可以进行整合 (用到再学)</p>\n<h3 id=\"redis的持久化\"><a class=\"markdownIt-Anchor\" href=\"#redis的持久化\">#</a> redis 的持久化</h3>\n<p>redis 是 nosql 数据库，需要把数据保存到磁盘。<br>\nredis 所有的数据都是保存在内存中，保存的数据量取决于内存的容量。<br>\nredis 提供了两种持久化机制：</p>\n<ul>\n<li>\n<p>RDB: 默认开启，快照模式</p>\n</li>\n<li>\n<p>AOF：日志存储，把对 redis 的操作的命令以日志方式存储到文件，当需要恢复数据时，从头到尾把命令执行一遍， 需要手动开启，</p>\n</li>\n<li>\n<p>注：如果同时开启了 RBD 和 AOF 默认是使用 aof 恢复数据。</p>\n</li>\n</ul>\n<h4 id=\"rdb默认使用\"><a class=\"markdownIt-Anchor\" href=\"#rdb默认使用\">#</a> RDB（默认使用）</h4>\n<p>RDB 方式是通过快照（ snapshotting ）完成的，当符合一定条件时 Redis 会自动将内存中的数据进行，快照并持久化到硬盘<br>\n执行时机：</p>\n<ol>\n<li>符合指定配置的快照规则</li>\n<li>执行 save 或 bgsave 命令 save 主线程去快照 bgsave 调用异步线程去快照<br>\n主线程是单线程 4.0 I/O 操作 已经有多线程概念</li>\n<li>执行 flushall 或 flushdb</li>\n<li>执行主从复制操作</li>\n</ol>\n<p>可以手动控制快照规则<br>\n save 多少秒内 数据变了多少<br>\n save “” : 不使用 RDB 存储<br>\n save 900 1 ： 表示 15 分钟（900 秒钟）内至少 1 个键被更改则进行快照。<br>\nsave 300 10 ： 表示 5 分钟（300 秒）内至少 10 个键被更改则进行快照。<br>\nsave 60 10000 ：表示 1 分钟内至少 10000 个键被更改则进行快照。</p>\n<p>快照过程：</p>\n<ol>\n<li>redis 调用系统中的 fork 函数复制一份当前进程的副本（子进程）</li>\n<li>父进程继续接收客户端的发来的命令，而子进程则开始将内存中的数据写入到硬盘中的临时文件，</li>\n<li>当子进程写完所有的数据后，会用临时文件替换掉旧的 rdb 文件，至此一次快照完成。</li>\n</ol>\n<p>rdb 的优缺点<br>\n优点：<br>\nRDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进<br>\n程，然后这个子进程就会处理接下来的所有保存工作，父进程无需执行任何磁盘 I/O 操作.</p>\n<p>缺点：使用 RDB 方式实现持久化，一旦 Redis 异常退出，就会丢失最后一次快照以后更改的所有数据，如果数据集比较大的时候， fork 可以能比较耗时，造成服务器在一段时间内停<br>\n止处理客户端的请求；</p>\n<h4 id=\"aof\"><a class=\"markdownIt-Anchor\" href=\"#aof\">#</a> AOF</h4>\n<p>默认情况下 Redis 没有开启 AOF （ append only file ）方式的持久化。<br>\n开启 AOF 持久化后，每执行一条会更改 Redis 中的数据的命令， Redis 就会将该命令写入硬盘中的 AOF 文件，这一过程会降低 Redis 的性能，但大部分情况下这个影响是能够接受的，另外使用较快的硬盘可以提高 AOF 的性能。<br>\nRedis 每次更改数据的时候， aof 机制都会将命令记录到 aof 文件，但是实际上由于操作系统的缓存机制，数据并没有实时写入到硬盘，而是进入硬盘缓存。再通过硬盘缓存机制去刷新到保存到文件。</p>\n<h4 id=\"混合持久化方式\"><a class=\"markdownIt-Anchor\" href=\"#混合持久化方式\">#</a> 混合持久化方式</h4>\n<p>这是在 4.0 之后的新版本中新增的。混合持久化是结合了 RDB 和 AOF 的优点，在写入的时候，先把当前的数据以 RDB 的形式写入文件的开头，再将后续的操作命令以 AOF 的格式存入文件，这样既能保证 Redis 重启时的速度，又能减低数据丢失的风险。<br>\n有两种开启方式：<br>\n1、通过命令行开启；<br>\n2、通过配置文件开启</p>\n<h2 id=\"redis-集群模式\"><a class=\"markdownIt-Anchor\" href=\"#redis-集群模式\">#</a> Redis 集群模式</h2>\n<h3 id=\"主从复制\"><a class=\"markdownIt-Anchor\" href=\"#主从复制\">#</a> 主从复制</h3>\n<h3 id=\"哨兵集群\"><a class=\"markdownIt-Anchor\" href=\"#哨兵集群\">#</a> 哨兵集群</h3>\n<h3 id=\"redis-cluster-集群\"><a class=\"markdownIt-Anchor\" href=\"#redis-cluster-集群\">#</a> Redis Cluster 集群</h3>\n<h4 id=\"集群介绍\"><a class=\"markdownIt-Anchor\" href=\"#集群介绍\">#</a> 集群介绍</h4>\n<h5 id=\"1-搭建主从\"><a class=\"markdownIt-Anchor\" href=\"#1-搭建主从\">#</a> 1、搭建主从</h5>\n<p>主节点：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">port 8001</span><br><span class=\"line\">daemonize no</span><br><span class=\"line\">protected-mode no</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-搭建分片集群\"><a class=\"markdownIt-Anchor\" href=\"#1-搭建分片集群\">#</a> 1、搭建分片集群</h5>\n<p>测试是在一台服务器上同时启动多个 redis 实例完成的，当然也可以使用多个服务器目前条件有限，<br>\n1）先下载安装一台单机的 redis</p>\n<ol>\n<li>安装 GCC 环境<br>\n yum install -y gcc-c++<br>\nyum install -y wget</li>\n<li>下载并解压缩 Redis 源码压缩包<br>\n wget <span class=\"exturl\" data-url=\"aHR0cDovL2Rvd25sb2FkLnJlZGlzLmlvL3JlbGVhc2VzL3JlZGlzLTUuMC40LnRhci5neg==\">http://download.redis.io/releases/redis-5.0.4.tar.gz</span><br>\ntar -zxf redis-5.0.4.tar.gz<br>\n3. 编译原码<br>\n cd redis-5.0.4<br>\nmake<br>\n4. 安装 Redis ，需要通过 PREFIX 指定安装路径，如果不指定默认是安装到 /usr/lcoal/bin 启动的适合不太方便<br>\n make install PREFIX=/kkb/server/redis<br>\n 这步做完最好把安装包里面的 redis.conf 文件复制一份到安装目录下的 bin 目录中，这样启动就方便很多<br>\n 5. 修改配置文件参数</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 将`daemonize`由`no`改为`yes`</span><br><span class=\"line\">daemonize yes</span><br><span class=\"line\"># 默认绑定的是回环地址，默认不能被其他机器访问</span><br><span class=\"line\"># bind 127.0.0.1</span><br><span class=\"line\"># 是否开启保护模式，由yes该为no</span><br><span class=\"line\">protected-mode no</span><br></pre></td></tr></table></figure>\n<p>6. 启动服务<br>\n./redis-server redis.conf<br>\n7. 关闭服务<br>\n./redis-cli shutdown<br>\n 这样单机就搭建完成了。下面是集群</p>\n<p>2）将 bin 目录下的数据持久化文件删掉，在启动集群前要保证是一台全新的 redis. 只保留以下 7 个文件</p>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230250244.png\" alt=\"image-20230905230250244\"><br>\n3) 将 bin 目录复制 6 份，分别命名 1-6，修改每个目录里面的 redi-conf 文件，将端口号分别改为 8801-8806<br>\n 将配置文件中的 设置为此 cluster-enable yes<br>\n 另外需要关闭防火墙，或者设置白名单，开启端口等等。不然多个服务器之间集群无法访问</p>\n<p>4） 启动所有的 redis，我这里写了一个脚本启动</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd bin-1</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br><span class=\"line\">cd bin-2</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br><span class=\"line\">cd bin-3</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br><span class=\"line\">cd bin-4</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br><span class=\"line\">cd bin-5</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br><span class=\"line\">cd bin-6</span><br><span class=\"line\">chmod 777 redis-server</span><br><span class=\"line\">./redis-server redis.conf</span><br><span class=\"line\">cd ..</span><br></pre></td></tr></table></figure>\n<p>5）进入 bin-1 中，使用以下命令，创建集群</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./redis-cli --cluster create 192.168.1.110:8001 192.168.1.110:8002 192.168.1.110:8003 192.168.1.110:8004 192.168.1.110:8005 192.168.1.110:8006 --cluster-replicas 1</span><br></pre></td></tr></table></figure>\n<p>最后的参数 1 表示 每个 redis 有一个备份 当主机挂了，备份定上，这也是为什么需要 6 个的原因，<br>\n6) 如下图，在过程中输入 yes 启动成功<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230303127.png\" alt=\"image-20230905230303127\"><br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230310340.png\" alt=\"image-20230905230310340\"><br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230318236.png\" alt=\"image-20230905230318236\"></p>\n<h4 id=\"2-连接集群\"><a class=\"markdownIt-Anchor\" href=\"#2-连接集群\">#</a> 2、连接集群</h4>\n<p>连接集群中任意一台机器都行，例如使用 cli 连接 8001 机器<br>\n./redis-cli -h 192.168.1.110 -p 8001 -c<br>\n 连接集群一定要加 - c 这个参数，不然插入会报错，因为集群模式下，每次新增键都需要进行插槽计算。如果用可视化也是需要集群模式连接<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230325571.png\" alt=\"image-20230905230325571\"><br>\n 使用 Java 去连接时，需要将所以的节点列出来，然后他会自己去选择连接</p>\n<h4 id=\"3-查看集群状态\"><a class=\"markdownIt-Anchor\" href=\"#3-查看集群状态\">#</a> 3、查看集群状态</h4>\n<h4 id=\"4-集群优缺点\"><a class=\"markdownIt-Anchor\" href=\"#4-集群优缺点\">#</a> 4、集群优缺点</h4>\n<p>客户端与 Redis 节点直连，不需要中间 Proxy 层，直接连接任意一个 Master 节点<br>\n根据公式 HASH_SLOT=CRC16 (key) mod 16384，计算出映射到哪个分片上，然后 Redis 会去相应的节<br>\n点进行操作<br>\n优点:<br>\n(1) 无需 Sentinel 哨兵监控，如果 Master 挂了，Redis Cluster 内部自动将 Slave 切换 Master<br>\n (2) 可以进行水平扩容<br>\n (3) 支持自动化迁移，当出现某个 Slave 宕机了，那么就只有 Master 了，这时候的高可用性就无法很好的保证<br>\n了，万一 Master 也宕机了，咋办呢？ 针对这种情况，如果说其他 Master 有多余的 Slave ，集群自动把多余<br>\n的 Slave 迁移到没有 Slave 的 Master 中。<br>\n缺点:<br>\n(1) 批量操作是个坑<br>\n (2) 资源隔离性较差，容易出现相互影响的情况。</p>\n<h2 id=\"redis数据存储细节\"><a class=\"markdownIt-Anchor\" href=\"#redis数据存储细节\">#</a> redis 数据存储细节</h2>\n<p>redis 的一个 DB 就是一个 HashTable,<br>\n 一个 hashtable 由 1 个 dict 结构、2 个 dictht 结构、1 个 dictEntry 指针数组（称为 bucket）和多个 dictEntry 结构组成<br>\n dictEntry 结构用于保存键值对，结构定义如下：<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230336488.png\" alt=\"image-20230905230336488\"></p>\n<h4 id=\"redis的对象类型与内存编码\"><a class=\"markdownIt-Anchor\" href=\"#redis的对象类型与内存编码\">#</a> redis 的对象类型与内存编码</h4>\n<p>Redis 支持 5 种对象类型，而每种结构都有至少两种编码</p>\n<ol>\n<li>\n<p>String<br>\n 字符串是最基础的类型，因为所有的键都是字符串类型，且字符串之外的其他几种复杂类型的元素也是字符串<br>\n字符串长度不能超过 512MB。有三种编码 分别是 int ;embstr ;raw; 数据比较少时使用 embstr 数据比较多时使用 raw<br>\nkey-value<br>\nSDS 结构体进行存储</p>\n</li>\n<li>\n<p>List<br>\n 列表（list）用来存储多个有序的字符串，每个字符串称为元素；<br>\n一个列表可以存储 2^64-1 个元素。<br>\nRedis 中的列表支持两端插入和弹出，并可以获得指定位置（或范围）的元素，可以充当数组、队列、栈等。<br>\nRedis3.0 之前列表的内部编码可以是压缩列表（ziplist）或双端链表（linkedlist）但是在 3.2 版本之后 因为转换也 是个费时且复杂的操作，引入了一种新的数据格式，结合了双向列表 linkedlist 和 ziplist 的特点，称之为 quicklist。有 的节点都用 quicklist 存储，省去了到临界条件是的格式转换。<br>\n压缩列表（ziplist）是 Redis 为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结<br>\n构，一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值，放到一个连续内存区。当一个列表只包含少量列表项时，并且每个列表项时小整数值或短字符串，那么 Redis 会使用压缩列表来做该列表的底层实现，<br>\n 目前使用的是 quicklist 我们仍旧可以将其看作一个双向列表，但是列表的每个节点都是一个 ziplist，其实就是<br>\n linkedlist 和 ziplist 的结合。quicklist 中的每个节点 ziplist 都能够存储多个数据元素。<br>\nRedis3.2 开始，列表采用 quicklist 进行编码。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905230344677.png\" alt=\"image-20230905230344677\"></p>\n</li>\n<li>\n<p>Hash</p>\n</li>\n</ol>\n<ul>\n<li>Redis 中内层的哈希既可能使用哈希表，也可能使用压缩列表。</li>\n<li>只有同时满足下面两个条件时，才会使用压缩列表：</li>\n</ul>\n<ol>\n<li>\n<p>哈希中元素数量小于 512 个；</p>\n</li>\n<li>\n<p>哈希中所有键值对的键和值字符串长度都小于 64 字节。</p>\n</li>\n<li>\n<p>Set<br>\n 但集合与列表有两点不同：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。<br>\nintSet: 集合中的元素都是数值类型<br>\n只有同时满足下面两个条件时，集合才会使用整数集合：</p>\n</li>\n<li>\n<p>集合中元素数量小于 512 个；</p>\n</li>\n<li>\n<p>集合中所有元素都是整数值。<br>\n如果有一个条件不满足，则使用哈希表；且编码只可能由整数集合转化为哈希表，反方向则不可能。</p>\n</li>\n<li>\n<p>ZSet<br>\n 有序集合的内部编码可以是压缩列表（ziplist）或跳跃表（skiplist）。<br>\n只有同时满足下面两个条件时，才会使用压缩列表：<br>\n1）有序集合中元素数量小于 128 个；<br>\n2）有序集合中 所有成员长度都不足 64 字节 。<br>\n如果有一个条件不满足，则使用跳跃表；且编码只可能由压缩列表转化为跳跃表，反方向则不可能。</p>\n</li>\n</ol>\n<h4 id=\"跳表-zskiplist\"><a class=\"markdownIt-Anchor\" href=\"#跳表-zskiplist\">#</a> 跳表 zskiplist：</h4>\n<p>类似于折半查找</p>\n<h2 id=\"redis性能优化简单学了点\"><a class=\"markdownIt-Anchor\" href=\"#redis性能优化简单学了点\">#</a> redis 性能优化（简单学了点）</h2>\n<h3 id=\"优化内存占用\"><a class=\"markdownIt-Anchor\" href=\"#优化内存占用\">#</a> 优化内存占用</h3>\n<ol>\n<li>利用 jemalloc 内存分配器（默认使用）</li>\n<li>能用整形 / 长整型的尽量使用，减少使用字符串</li>\n<li>利用共享对象，引用常量池</li>\n<li>缩短键值对的存储长度（减少 key 的长度）</li>\n</ol>\n<h3 id=\"性能优化\"><a class=\"markdownIt-Anchor\" href=\"#性能优化\">#</a> 性能优化</h3>\n<ol>\n<li>设置键值的过期时间</li>\n<li>使用 lazy free 特性，（惰性删除），不是马上删掉，而是放到删除队列里面，一起删除，或者是开子线程删除。</li>\n<li>限制 redis 内存大小，设置内存淘汰策略</li>\n<li>禁用长耗时的查询命令</li>\n<li>使用 slowlog 优化耗时命令</li>\n<li>避免大量数据同时失效</li>\n<li>使用 Pipeline 批量操作数据</li>\n<li>客户端使用连接池优化</li>\n<li>使用分布式架构来增加读写速度</li>\n<li>禁用 THP 特性 /*</li>\n</ol>\n",
            "tags": [
                "原理",
                "学习笔记"
            ]
        },
        {
            "id": "https://zhai-xing.github.io/hexo-blog/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0",
            "url": "https://zhai-xing.github.io/hexo-blog/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0",
            "title": "MySQL学习笔记",
            "date_published": "2023-09-04T14:46:36.000Z",
            "content_html": "<h1 id=\"一-mysql基础篇\"><a class=\"markdownIt-Anchor\" href=\"#一-mysql基础篇\">#</a> 一、 Mysql 基础篇</h1>\n<h4 id=\"语句执行流程\"><a class=\"markdownIt-Anchor\" href=\"#语句执行流程\">#</a> 语句执行流程</h4>\n<h5 id=\"mysql查询语句执行流程\"><a class=\"markdownIt-Anchor\" href=\"#mysql查询语句执行流程\">#</a> mysql 查询语句执行流程：</h5>\n<ol>\n<li>建立连接：这里需要验证身份和权限</li>\n<li>查询缓存 (如果有缓存则直接返回缓存数据，如果木有则进行下一步，需要注意，如果有增删改操作会清除掉缓存)</li>\n<li>解析器：先做词法分析、再做语法分析 (注：表不存在或者字段不存在，并不是在解析器)</li>\n<li>执行器： 执行器有三个阶段，预处理阶段、优化阶段、执行阶段</li>\n<li>返回数据</li>\n</ol>\n<ul>\n<li>\n<p>词法分析主要是识别关键字构成语法树，这样方便后面的模块得到关键字、表名</p>\n</li>\n<li>\n<p>语法分析主要是分析语法是否正确</p>\n</li>\n<li>\n<p>预处理阶段：检查表名、列名是否存在 将 * 号换成所有列</p>\n</li>\n<li>\n<p>优化阶段：优化器主要是将 sql 语句的执行计划确定下来，例如索引的选择、</p>\n</li>\n<li>\n<p>执行阶段：执行器就会和存储引擎交互了，交互是以记录为单位的。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225131183.png\" alt=\"image-20230905225131183\"><br>\nmysql 更新语句执行流程：<br>\n更新语句同样需要走一次查询数据的流程，但数据的更新是在存储引擎中做的，在本文中存储引擎选择的是 Innodb。更新流程需要有日志的辅助，</p>\n</li>\n<li>\n<p>重做日志: redo Log 是物理日志，记录了某个数据页做了什么修改，每当执行一个事务就会产生一条或者多条物理日志。<mark>事务提交时</mark>，先将 redo log 持久化到磁盘即可。并且 redo 是可重用的，也就是说空间大小是可设置的，从头开始写，写到末尾又回到开头写， redo log 解决了事务中的持久性。这里到了一个 WAL 技术，<br>\n<mark>WAL 技术指的是，MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。</mark></p>\n</li>\n<li>\n<p>回滚日志: undo log 是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。<br>\n<mark>每开始一个事务都会分配一个 undo 空间，在事务没提交之前 Innodb 会先记录更新前的数据到 undo log 中</mark>，当需要执行回滚时 就执行一条相反的操作。undo log 有两个参数：roll_pointer 指针和一个 trx_id 事务 id，通过 trx_id 可以知道该记录是被哪个事务修改的；通过 roll_pointer 指针可以将这些 undo log 串成一个链表，形成版本链。<br>\n<mark>当事务提交后 相关的 undo 日志记录会被标记为 &quot;已提交&quot;，这些已提交的 undo 日志记录会在后续的清理过程中被回收和删除</mark><br>\n<mark>实现事务回滚，保障事务的原子性：如果出现了错误或者用户执行了 ROLLBACK 语句，可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。</mark></p>\n</li>\n<li>\n<p>归档日志 ：Server 层生成的日志，主要用于数据备份和主从复制。binlog 是 server 层的日志，不同的存储引擎都可用， 而上面两个是 innoDB 独有的，在完成一条更新操作后，Server 层会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写入 binlog 文件。binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作。<br>\n1. 执行器负责具体执行，会调用存储引擎的接口，通过索引获取到要操作的数据的记录</p>\n</li>\n</ul>\n<ol start=\"2\">\n<li>执行器得到聚簇索引记录后 会查看更新前和更新后的数据是否一致，如果一致就不执行。</li>\n<li>开启事务，innodb 在执行操作前需要先开启事务，InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来</li>\n<li>innodb 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了</li>\n<li>至此一条记录更新完成了</li>\n<li>在一条更新语句执行完成后，然后开始记录该语句对应的 binlog</li>\n<li>事务提交，事务提交分为两个阶段\n<ul>\n<li>prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；</li>\n<li>commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；<br>\n8. 至此，一条更新语句执行完成。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"二-日志篇\"><a class=\"markdownIt-Anchor\" href=\"#二-日志篇\">#</a> 二、日志篇</h1>\n<p>mysql 日志文件分为三种:</p>\n<h3 id=\"undo-log-回滚日志\"><a class=\"markdownIt-Anchor\" href=\"#undo-log-回滚日志\">#</a> undo log 回滚日志</h3>\n<p>回滚日志是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要是用于事务回滚和 MVCC, 考虑一个问题，一个事务在执行过程中还没有提交事务，mysql 发生了崩溃，就需要 undo log 来回滚到事务之前的数据去。<br>\nundo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225157734.png\" alt=\"image-20230905225157734\"><br>\n 原理：<br>\n每当 innodb 引擎对一条记录进行操作时，要把回滚时需要的信息都记录到 undolog 里，发生回滚时就读取 undolog 的数据，做相反操作。不同的操作，记录的内容也不一样<br>\n另外 undolog +readview 实现了 MVCC：</p>\n<ol>\n<li>对于读提交隔离级别是在每个 select 都会生成一个新的 read view 也意味着事务期间多次读取同一个数据 前后两次读的数据可能会出现不一致，因为另外一个事务修改了记录并提交了</li>\n<li>对于可重复读级别，是启动事务时生成了一个 read view 然后整个事务期间都在用整个 read view 这样就保证了事务期间读到的数据都是事务启动前的记录。<br>\n<mark>undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。</mark></li>\n</ol>\n<h3 id=\"redo-log-重做日志\"><a class=\"markdownIt-Anchor\" href=\"#redo-log-重做日志\">#</a> redo log 重做日志</h3>\n<p>为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候 innodb 引擎会先更新内存，然后将本次对整个页的修改以 redolog 的形式记录下来，就算更新完成了，后续 innodb 会在适当的时候由后台线程将缓存在 BufferPool 的脏页刷新到磁盘，这就是 WAL 技术，WAL 技术是指 MYSQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写入到磁盘</p>\n<ol>\n<li>redolog 有自己的内存缓存，</li>\n<li>redolog 有刷盘机制可手动配置</li>\n<li>InnoDB 存储引擎有 1 个重做日志文件组 ( redo log Group） 是一个文件组写完了可以循环写，防止丢失</li>\n</ol>\n<h4 id=\"undo和-redo的区别\"><a class=\"markdownIt-Anchor\" href=\"#undo和-redo的区别\">#</a> undo 和 redo 的区别</h4>\n<p>这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于：<br>\nredo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值；<br>\nundo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值；<br>\n<mark>事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，</mark></p>\n<h3 id=\"binglog-归档日志\"><a class=\"markdownIt-Anchor\" href=\"#binglog-归档日志\">#</a> binglog 归档日志</h3>\n<p>binglog 主要是做备份和主从复制的，并且 binglog 是 server 层的日志，是全量日志，<br>\n而前面俩都是 Innodb 的日志，换一个存储引擎就没有了。binlog 是追加写，一个文件写满了就创建一个新的继续写，不会覆盖日志，但是 redolog 会覆盖，<br>\nbinglog 日志有三种格式</p>\n<ol>\n<li>StateNet: 每一条修改数据的 sql 都会被记录到 binlog 中，，主从复制时，可以直接运行语句复现，STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；</li>\n<li>row：记录行数据最终被修改的样子，<br>\n4.mixed 包含了 statement 和 row 模式，会根据不同情况自动使用上面的两种日志</li>\n</ol>\n<h1 id=\"三-mysql事务\"><a class=\"markdownIt-Anchor\" href=\"#三-mysql事务\">#</a> 三、MySQL 事务</h1>\n<p>插入一条数据发生了什么？<br>\n使用 Innodb 引擎时执行增删改操作时，会自动在 Innodb 引擎层开启事务。<br>\nACID</p>\n<ul>\n<li>原子性：事务最小工作单元，要么全成功，要么全失败（重做日志了实现的）</li>\n<li>一致性：事务开始和结束后，数据库的完整性不会被破坏</li>\n<li>隔离性：不同事务之间互不影响，（是通过 MVCC 来实现的）\n<ol>\n<li>读未提交 RU：一个事务读取到另一个事务未提交的数据。（会出现脏读）</li>\n<li>读已提交（RC)：一个事务读取到另一个事务已经提交的数据，（会出现不可重复读，同一个 sql 语句在一个事务里面读到的数据不一致）</li>\n<li>可重复读 (RR)：一个事务只能读到另一个已经提交的事务修改的数据（会出现幻读，一个事务因读取到另一个事务已提交的 insert 数据或者 dlete 数据，导致对同一张表读取两次以上的结果不一致。查询结果是多条记录时，才有可能出现幻读。）</li>\n<li>串行化：</li>\n</ol>\n</li>\n<li>持久性：事务提交后，对数据的修改是永久性的，即使系统故障也不会丢失。（回滚日志来实现的）</li>\n</ul>\n<h3 id=\"并行事务会引发什么问题\"><a class=\"markdownIt-Anchor\" href=\"#并行事务会引发什么问题\">#</a> 并行事务会引发什么问题</h3>\n<ul>\n<li>脏读：一个事务读到了另一个未提交事务修改过的数据</li>\n<li>不可重复读：一个事务内两次读取数据出现不一致的情况，</li>\n<li>幻读：在一个事务内多次查询某个符合查询条件的记录数量，如果出现前后两次查询到的记录数量不一致，就是幻读。</li>\n</ul>\n<h3 id=\"事务与mvcc底层原理详解\"><a class=\"markdownIt-Anchor\" href=\"#事务与mvcc底层原理详解\">#</a> 事务与 MVCC 底层原理详解</h3>\n<p>MVCC 实现了两个事务隔离级别，可重复读和读已提交，MVCC 使数据库读不加锁，提高了数据库的并发处理能力，多版本并发控制仅仅是一种技术理念，没有统一的标准，其核心理念就是快照，不同的事务访问不同版本的数据快照，从而实现不同的事务隔离级别。<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225220682.png\" alt=\"image-20230905225220682\"></p>\n<ul>\n<li>\n<p>m_ids ：指的是在创建 Read View 时，当前数据库中活跃事务的事务 id 列表，活跃事务指的就是，启动了但还没提交的事务。</p>\n</li>\n<li>\n<p>min_trx_id ：指的是在创建 Read View 时，当前数据库中活跃事务中事务 id 最小的事务，也就是 m_ids 的最小值。</p>\n</li>\n<li>\n<p>max_trx_id ：创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1。</p>\n</li>\n<li>\n<p>creator_trx_id ：指的是创建该 Read View 的事务的事务 id。</p>\n</li>\n<li>\n<p>如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。</p>\n</li>\n<li>\n<p>如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。</p>\n</li>\n<li>\n<p>如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：</p>\n<ul>\n<li>如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着 (还没提交事务)，所以该版本的记录对当前事务不可见。</li>\n<li>如果记录的 trx_id 不在 m_ids 列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"如何解决幻读\"><a class=\"markdownIt-Anchor\" href=\"#如何解决幻读\">#</a> 如何解决幻读</h3>\n<p>分为快照读和当前读</p>\n<h4 id=\"快照读\"><a class=\"markdownIt-Anchor\" href=\"#快照读\">#</a> 快照读</h4>\n<p>通过 MVCC 方式解决幻读，可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一直的。即使中途有其他事务写入了一条数据，也是查不出来的。<br>\n<mark>对于快照读， MVCC 并不能完全避免幻读现象</mark>。<mark>当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读</mark></p>\n<h4 id=\"当前读\"><a class=\"markdownIt-Anchor\" href=\"#当前读\">#</a> 当前读</h4>\n<p>通过 next-key lock 方式解决了幻读，因为当执行 select for update 语句的时候会加上 next key lock 如果有其他事务在锁范围内插入了已经语句 这个插入语句就会被阻塞.</p>\n<h4 id=\"lbcc-基于锁的并发控制\"><a class=\"markdownIt-Anchor\" href=\"#lbcc-基于锁的并发控制\">#</a> LBCC 基于锁的并发控制。</h4>\n<p>一个事务去读一个数据库的时候就加上锁，不允许其他事务操作</p>\n<h4 id=\"innodb的mvcc实现\"><a class=\"markdownIt-Anchor\" href=\"#innodb的mvcc实现\">#</a> Innodb 的 MVCC 实现</h4>\n<p>MVCC 在 mysql 中的实现是依赖的 undo log 和 read view<br>\nMVCC 只支持两种隔离级别 分别是：读已提交，可重复读<br>\n根据不同的行为，undo log 分为两种 insert undo log 和 update undo log<br>\ninsert undo log 是在 inser 操作下产生的 undo log<br>\n 因为 insert 操作的记录只对事务本身可见，对于其他事务</p>\n<h4 id=\"读提交的实现\"><a class=\"markdownIt-Anchor\" href=\"#读提交的实现\">#</a> 读提交的实现</h4>\n<p>读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务</p>\n<h1 id=\"四-mysq锁篇\"><a class=\"markdownIt-Anchor\" href=\"#四-mysq锁篇\">#</a> 四、Mysq 锁篇</h1>\n<h3 id=\"锁介绍\"><a class=\"markdownIt-Anchor\" href=\"#锁介绍\">#</a> 锁介绍</h3>\n<p>范围分：全局锁，表级锁，行级锁<br>\n根据功能分：共享锁（s)，排他锁 (x)，<br>\n全局锁就是对整个数据库实例枷锁，加锁后整个数据库就处于只读状态，后续的 MDL ，DDL 语句 和已经更新操作的提交语句都将被阻塞，一般应用于数据库全局备份时，保证数据完整性和一致性。<br>\n加锁命令：flush table with read lock;<br>\n 释放锁 unlock tables;<br>\n 一般是在数据库做备份的时候才使用全局锁，但是会带来问题，全局锁会导致数据库在时间内处于只读状态，会导致业务停滞，<br>\n解决方案，如果数据库引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前可先开启事务，会先创建 ReadView 然后整个事务执行期间都在使用这个 ReadView ，而且在可重复读的情况下，即使其他业务更新数据库，也不会有影响。</p>\n<hr>\n<p>表级锁 (server 实现的): 有四种 读锁，写锁，元数据锁</p>\n<ol>\n<li>表级共享锁：lock table 表名 read;</li>\n<li>表级排他锁: lock table 表名 write</li>\n<li>元数据锁： 在一个事务中对一个表进行查询操作不允许其他会话对表结构进行修改，就在表上加元数据锁</li>\n<li>自增锁：使用自增字段时，使用自增主键保证主键不冲突</li>\n<li>意向锁：当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。<br>\n那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录<br>\n意向锁的目的是为了快速判断表里释放有记录被加锁</li>\n</ol>\n<hr>\n<p>行级锁<br>\n<strong>要求必须使用 Innodb 引擎</strong></p>\n<ul>\n<li>记录锁：锁定索引中的一条记录</li>\n<li>间隙锁：要么锁住索引记录中间的值，要么锁住第一个索引记录前面的值或者最后一个索引记录后面的值</li>\n<li>临健锁：是索引记录上的记录锁和在索引记录之前的间隙锁的组合（间隙锁 + 记录锁）</li>\n<li>插入意向锁： 做 insert 操作时添加的对记录 id 的锁<br>\n记录锁：根据主键等值更新时使用记录锁<br>\n共享记录锁: select * from where id=1 lock inshare mode<br>\n 排他记录锁: select * from where id=1 for update;<br>\n 意向锁就是一个标志位，表示当前表中，是否有行锁<br>\n意向锁分为：行锁：S 意向锁：IS; 行锁 X 意向锁：IX<br>\n 间隙锁：仅仅锁住一个索引区间，在记录和记录之间的范围区间就是间隙，间隙锁就是加在区间之上，防止插入数据，目的就是幻读 (读一个范围数据，读到了其他数据插入的数据) 在更新过程中，不仅需要对记录加锁还需要在记录与记录之间加锁，加的就是间隙锁。<br>\n临键锁：就是记录锁 + 间隙锁，是一个左开右闭区间</li>\n</ul>\n<h3 id=\"死锁的产生\"><a class=\"markdownIt-Anchor\" href=\"#死锁的产生\">#</a> 死锁的产生</h3>\n<p>有两个会话，互相持有对方所需要的资源。</p>\n<h4 id=\"如何避免死锁\"><a class=\"markdownIt-Anchor\" href=\"#如何避免死锁\">#</a> 如何避免死锁・</h4>\n<p>1、注意程序逻辑，最常见的就是更新表，程序的更新过程最好意思一致的<br>\n 2、保持事务的轻量，越是轻量的事务，占有越少的锁资源，这样发生死锁的概率就很低了<br>\n 3、提高运行速度，避免使用子查询，尽量使用主键等<br>\n 4、尽快提交事务，减少持有锁的时间，事务越早提交，锁就越早释放。</p>\n<h1 id=\"五-索引\"><a class=\"markdownIt-Anchor\" href=\"#五-索引\">#</a> 五、索引</h1>\n<p>什么是索引，帮助快速查找数据的一个数据结构。索引可以说是数据的目录，<br>\n<img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225256269.png\" alt=\"image-20230905225256269\"><br>\n 索引分类<br>\n按数据结构来分，可以分为，B + 树索引，Hash 索引，Full-text 索引<br>\n按物理存储分类：聚簇索引 (主键索引)，二级索引 (辅助索引)<br>\n 按字段特性分类： 主键索引、唯一索引、普通索引、前缀索引<br>\n按字段个数分类： 单列索引，联合索引。<br>\n如果有主键，默认会使用主键作为聚簇索引的索引键（key）；<br>\n如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）；<br>\n在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）；</p>\n<h4 id=\"什么时候需要索引\"><a class=\"markdownIt-Anchor\" href=\"#什么时候需要索引\">#</a> 什么时候需要索引：</h4>\n<p>字段有唯一性限制的<br>\n经常用于 where 查询条件的字段，<br>\n经常用于 group by 和 order by 的字段，这样查询的时候就不需要再去做一次排序了，</p>\n<h4 id=\"什么时候不需要创建索引\"><a class=\"markdownIt-Anchor\" href=\"#什么时候不需要创建索引\">#</a> 什么时候不需要创建索引</h4>\n<p>字段中有大量重复数据的，比如性别<br>\n表数据很少的时候<br>\n经常更新的字段不需要索引，比如余额</p>\n<h4 id=\"优化索引的方法\"><a class=\"markdownIt-Anchor\" href=\"#优化索引的方法\">#</a> 优化索引的方法</h4>\n<ol>\n<li>\n<p><strong>前缀索引优化</strong><br>\n就是使用某个字段中字符串的前几个字符建立索引，使用前缀索引可以减少索引字段的大小，但是 order by 无法使用前缀索引，无法把前缀索引当做覆盖索引使用</p>\n</li>\n<li>\n<p><strong>覆盖索引优化</strong><br>\n在索引 B + 树的叶子节点上都能找到的那些索引。可以建立联合索引，商品 id 姓名，作为一个联合索引，可以避免回表。</p>\n</li>\n<li>\n<p><strong>主键索引最好自增</strong><br>\n那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据</p>\n</li>\n<li>\n<p><strong>索引最后设置为 NOT NULL</strong><br>\n 如果索引列存在 NULL 会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为 NULL 的行。<br>\nNULL 值是一个没意义的值，但是它会占用物理空间</p>\n</li>\n<li>\n<p><strong>防止索引失效</strong><br>\n 1. 当使用左或者左右模糊匹配的时候，也就是 like % xx 或者 like % xx% 这两种方法都会造成索引失效。</p>\n</li>\n<li>\n<p>当我们在查询条件中对索引列做了计算、函数、类型转换后也会导致失效。</p>\n</li>\n<li>\n<p>联合索引要遵循最左匹配原则不然也会失效。</p>\n</li>\n<li>\n<p>where 子句中，如果 or 前的条件列是索引，而 or 后的条件不是所有，那么也会导致索引失效。</p>\n</li>\n</ol>\n<p>All（全表扫描）<br>\nindex（全索引扫描）<br>\nrange（索引范围扫描）<br>\nref（非唯一索引扫描）<br>\neq_ref（唯一索引扫描）<br>\nconst（结果只有一条的主键或唯一索引扫描）</p>\n<h4 id=\"count-是什么\"><a class=\"markdownIt-Anchor\" href=\"#count-是什么\">#</a> count () 是什么？</h4>\n<p>count () 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个。</p>\n<h3 id=\"索引的数据结构\"><a class=\"markdownIt-Anchor\" href=\"#索引的数据结构\">#</a> 索引的数据结构</h3>\n<p>Mysql 的索引数据结构选用的是 B + 树，为什么选用呢？<br>\n索引至少要支持等值查询和范围查询。</p>\n<h4 id=\"选择hash表做查询\"><a class=\"markdownIt-Anchor\" href=\"#选择hash表做查询\">#</a> 选择 Hash 表做查询</h4>\n<p>如果是等值查询，hash 的性能是很好的，但是无法做范围查询，空间复杂度较高。</p>\n<h4 id=\"二叉查找树\"><a class=\"markdownIt-Anchor\" href=\"#二叉查找树\">#</a> 二叉查找树</h4>\n<p>这个是可以做等着查询和范围查询的，但是极端情况下，可能会退化成链表。</p>\n<h4 id=\"平衡二叉查找树\"><a class=\"markdownIt-Anchor\" href=\"#平衡二叉查找树\">#</a> 平衡二叉查找树</h4>\n<p>查找的时间复杂度和树的高度有关，树有多高就需要查找多少次，每个节点的读取都对应着一次 IO 操作，在表数据量变大时，树的高度也会变大，查询效率下降严重<br>\n并且平衡二叉查找树，不支持范围快速查找，范围查询时，需要通过根结点多次遍历。查询效率不高</p>\n<h4 id=\"b树\"><a class=\"markdownIt-Anchor\" href=\"#b树\">#</a> B 树</h4>\n<p>改造了二叉树，在一个节点上存多个数值，可以将树的高度变矮，降低磁盘的读取 IO;<br>\nB 树是一种多叉平衡查找树</p>\n<p>B 树：非叶子节点和叶子节点都会存储数据。<br>\nb 树缺点：</p>\n<ol>\n<li>B 树不支持范围查询的快速查找，如果我们想要查找 15 和 26 之间的数据，查找到 15 之后，需要回到<br>\n根节点重新遍历查找，需要从根节点进行多次遍历，查询效率有待提高。</li>\n<li>如果 data 存储的是行记录，行的大小随着列数的增多，所占空间会变大。这时，一个页中可存储的<br>\n数据量就会变少，树相应就会变高，磁盘 IO 次数就会变大</li>\n</ol>\n<h4 id=\"b树-2\"><a class=\"markdownIt-Anchor\" href=\"#b树-2\">#</a> B + 树</h4>\n<p>而 B + 树只在叶子节点上存储数据，且使用链表将叶子节点连接起来。</p>\n<h4 id=\"为什么用b树不用b树\"><a class=\"markdownIt-Anchor\" href=\"#为什么用b树不用b树\">#</a> 为什么用 B + 树不用 B 树</h4>\n<p>B + 树相对于 B 树在磁盘 IO 操作上具有优势，尤其适用于范围查询和索引场景</p>\n<p>在进行查询操作时，B 树可能需要在内部节点上进行多次访问才能达到叶子节点，而 B + 树由于只有叶子节点存储数据，所以查询时只需一次访问。</p>\n<h3 id=\"mysql索引实现\"><a class=\"markdownIt-Anchor\" href=\"#mysql索引实现\">#</a> mysql 索引实现</h3>\n<h4 id=\"myisam引擎\"><a class=\"markdownIt-Anchor\" href=\"#myisam引擎\">#</a> MyIsam 引擎</h4>\n<p>主键索引实现是使用了 B + 树，MyISAM 在查询时，会将索引节点缓存在 MySQL 缓存中，而数据缓存依赖于操作系统自身的缓存。</p>\n<h4 id=\"innodbt引擎\"><a class=\"markdownIt-Anchor\" href=\"#innodbt引擎\">#</a> Innodbt 引擎</h4>\n<p>索引和数据文件是放在一起的，Innodb 引擎要求每张表必须有主键索引，<br>\n每个 InnoDB 表都有一个聚簇索引 ，聚簇索引使用 B + 树构建，叶子节点存储的数据是整行记录。一般<br>\n情况下聚簇索引等同于主键索引，当一个表没有创建主键索引时，InnoDB 会自动创建一个 ROWID 字<br>\n段来构建聚簇索引。</p>\n<p>在使用辅助索引时，数据会回表，除聚簇索引之外的所有索引都称为辅助索引，InnoDB 的辅助索引只会存储主键值而非磁盘地址，<br>\n使用辅助索引需要检索两遍索引：首先检索辅助索引获得主键，然后使用主键到主索引中检索获得记<br>\n录。根据在辅助索引树中获取的主键 id，到主键索引树检索数据的过程称为回表查询。回表会有性能损耗，所以也有说在 Innodb 引擎里使用辅助索引，性能不如 MyIsam 引擎，因为在 Mysaml 里面辅助索引和主索引是相同的。</p>\n<h5 id=\"innodb组合索引\"><a class=\"markdownIt-Anchor\" href=\"#innodb组合索引\">#</a> Innodb 组合索引</h5>\n<p>组合索引就是一个字段包含多个索引。<br>\n如何存储的呢，是根据创建索引时的字段先后顺序，假设组合索引是 (a,b,c) , 那么在建立索引存储结构 B + 树时，就先按照 a 字段排序，当 a 字段相同时 ，就按 b 字段排序，当 b 字段相同时，就按 c 字段排序，在写 sql 语句的时候，要按照索引创建的顺序去写条件</p>\n<h5 id=\"最左前缀匹配原则\"><a class=\"markdownIt-Anchor\" href=\"#最左前缀匹配原则\">#</a> 最左前缀匹配原则</h5>\n<h4 id=\"什么情况下适合建立索引\"><a class=\"markdownIt-Anchor\" href=\"#什么情况下适合建立索引\">#</a> 什么情况下适合建立索引</h4>\n<ol>\n<li>频繁出现在 where 条件 order 排序 group 分组 字段中的列</li>\n<li>select 频繁查询的列，考虑是否创建联合索引</li>\n<li>多表 join 关联查询 ，on 两边的字段都应该创建索引</li>\n</ol>\n<h4 id=\"索引优化\"><a class=\"markdownIt-Anchor\" href=\"#索引优化\">#</a> 索引优化</h4>\n<ol>\n<li>表记录很少不需创建索引 （索引是要有存储的开销）</li>\n<li>频繁更新的字段不适合创建索引</li>\n<li>区分度低的字段不适合建立索引，例如性别，会导致扫描行数过多，再加上回表查询的消耗。如果使用索引，比全表扫描的性能还要差。这些字段一般会用在组合索引中。</li>\n<li>在 InnoDB 存储引擎中，主键索引建议使用自增的长整型，避免使用很长的字段。</li>\n<li>不建议用无序的值作为索引。例如身份证、UUID，更新数据时会发生频繁的页分裂，页内数据不紧凑，浪费磁盘空间。</li>\n<li>尽量创建组合索引，而不是单列索引，一个组合索引等于多个单列索引<br>\n创建原则：组合索引应该把把频繁的列，区分度高的值放在前面。频繁使用代表索引的利用率高，<br>\n区分度高代表筛选粒度大，可以尽量缩小筛选范围</li>\n</ol>\n<h1 id=\"六-架构篇\"><a class=\"markdownIt-Anchor\" href=\"#六-架构篇\">#</a> 六、架构篇</h1>\n<h4 id=\"mysql文件类型\"><a class=\"markdownIt-Anchor\" href=\"#mysql文件类型\">#</a> mysql 文件类型：</h4>\n<p>日志文件：<br>\n错误日志<br>\n通用日志：默认是关闭的，可以手动开启，，通用查询日志会记录用户所有的操作，<br>\nbinlog: 二进制日志，做 Mysql 主从复制时使用<br>\n慢查询日志: mysql 调优时使用，默认关闭，<br>\n数据文件：<br>\n记录表结构、数据、索引信息<br>\n InnoDB ，MyIsam 等<br>\n不同的存储引擎，生成的文件也不一样<br>\n在文件管理系统中 mysql 的 一个数据库对应一个目录，<br>\nfrm 文件类型：表结构定义文件<br>\n MYD 文件 Mylasm 引擎创建的表，存储表中数据<br>\n MYI: 文件 Mylasm 引擎创建的表，存储表中索引<br>\n ibd 文件 InnoDB 引擎创建的表，其中包含表中数据和索引<br>\n MyIsam 引擎创建的表有三个文件<br>\n InnoDB 引擎创建的表有两个文件<br>\n同一个数据库中，不同的表可以使用不同的数据引擎，Mysql5.5 开始默认使用 InnoDB 之前是使用的 mylsam，</p>\n<h4 id=\"mysql架构\"><a class=\"markdownIt-Anchor\" href=\"#mysql架构\">#</a> MYSQL 架构：</h4>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">大致分为两层:</span><br><span class=\"line\">mysqlServer层: 连接池、优化器、执行器等组件</span><br><span class=\"line\">存储引擎层： Innodb，以及其他引擎，基本可以忽略 </span><br></pre></td></tr></table></figure>\n<p>![](…/…/B69V2_(NQHU5$@Q[S18C9S9%201.png)<br>\n Mysql8 中缓存被移除了，在 mysql5.7 中依然存在，默认是关闭的，<br>\n缓存的缺点：<br>\n1、需要占用大量的内存空间<br>\n 2、维护缓存需要成本。<br>\n3、命中率不高<br>\n在同一个 sql 语句会反复执行时，可以开启，可以使用命令查看缓存命中率</p>\n<p>分析器：<br>\n先对 sql 进行词法分析，语法分析<br>\n优化器：<br>\n执行器：</p>\n<p>存储引擎<br>\n Mysql 有多个自带存储引擎，也可以自己去安装第三方的存储引擎，<br>\n使用 show engines 查看 mysql 中自带的存储引擎<br>\n<strong>除非需要用到某些 Innodb 不具备的特性，并且没有其他办法可以替代，否则都应该选择 Innodb 引擎</strong><br>\n MEMORY ：内存引擎。把数据和索引全部放到内存中。缺点是，数据库一旦重启，表数据就丢失了，表结构不会丢失；优点速度快，</p>\n<p>Innode 引擎：<br>\n由，内存池，后台线程，磁盘文件三大部分组成，<br>\n内存结构:<br>\nredo log buffer（重做日志缓冲区) 保证 Mysql 数据库完整性的重要组成部分<br>\n buffer poll 缓冲池<br>\n数据页缓存<br>\n索引页缓存<br>\n change buffer 修改缓冲区（插入缓冲区），修改数据之后需要更新辅助索引，为了提高性能暂时先不更新，把要更新的操作放到缓冲区（change buffer）中。<br>\n自适应哈希索引：完全由 mysql 管理，无法人工干预，<br>\n磁盘结构：<br>\n1・系统表空间：ibdata1<br>\n 数据字典<br>\n双写缓冲区<br>\n修改缓冲区，内存中的 changebuffer 的映射<br>\n回滚段，undolog<br>\n2、用户表空间（每个表一个）<br>\n默认情况下，每个表都对应一个 ibd 文件，就是用户表空间，用户的数据和索引都保存在用户表空间中<br>\n 3、通用表空间<br>\n默认没有这个文件的，在 mysql 中，使用 create tablespace 命令创建的表空间就通用表空间。<br>\n4、回滚表空间<br>\n默认是没有的，undo tablespace 默认是在系统表空间中，<br>\n5、临时表空间，<br>\n默认也是没有的，当使用临时表时才出现。<br>\n6、redolog 重做日志文件<br>\n就是 mysql 数据库数据完整性的重要保障文件。<br>\n由一组文件组成：<br>\nib logfile0<br>\niblogfile1<br>\n 两个文件循环使用，文件不会增长。</p>\n<h2 id=\"二-innodb架构组织\"><a class=\"markdownIt-Anchor\" href=\"#二-innodb架构组织\">#</a> 二、Innodb 架构组织</h2>\n<h3 id=\"innodb磁盘文件结构\"><a class=\"markdownIt-Anchor\" href=\"#innodb磁盘文件结构\">#</a> Innodb 磁盘文件结构</h3>\n<p>InnoDB 的主要的磁盘文件主要分为三大块：一是系统表空间，二是用户表空间，三是 redo 日志文件和归档文件<br>\n三个表空间的区别，redo 是日志：保证 mysql 数据不丢失的重要环节，采用 WAL 顺序写实现，系统表空间和用户表空间采用的就是随机写，</p>\n<ol>\n<li>redolog 文件:<br>\n 由一组文件组成，默认是两个文件:ib_logfile0 和 ib_logfile1, 循环写入，文件 1 写满之后写文件 2，文件 2 写满之后写文件 1，会覆盖之前的内容。</li>\n<li>系统表空间：<br>\n文件中 ibdata1, 这就是系统表空间文件，其中包含<br>\n 1. 数据字典<br>\n 2. 双写缓冲区<br>\n 3. 修改缓冲区<br>\n 4. 回滚日志</li>\n<li>用户表空间：<br>\n默认每个表对应一个表空间文件。*.ibd 文件，是可配置的，由 innodb_file_per_table 参数控制默认是 true. 其中包含表中的数据和索引信息。<br>\n用户表空间结构：<br>\nibd 表空间文件<br>\n分段： 段下面分成若干个区，每个区分成若干页（默认 16k）（数据读写以页为单位，页里面存储的数据行，行大小取决于表结构），数据页的大小可以通过参数 innode_page_size 来进行调整；</li>\n</ol>\n<p>而通用表空间和临时表空间用的不多。</p>\n<p><img data-src=\"https://blog-1259743669.cos.ap-chengdu.myqcloud.com/image-20230905225348775.png\" alt=\"image-20230905225348775\"></p>\n<h4 id=\"1-innodb的内存结构\"><a class=\"markdownIt-Anchor\" href=\"#1-innodb的内存结构\">#</a> 1、Innodb 的内存结构</h4>\n<p>1、redolog buffer ：为了减少磁盘的 io，尽量将 redolog 相关的内容先写到缓冲区中，然后在合适的时机，将缓冲区的数据写到磁盘，合适的时机，就是 commit 操作，在 commit 之前，先把 redologBuffer 中的数据写入到 redolog 文件，如果写入成功那么 commit 成功，如果写入失败则 commit 失败。<br>\n[innodb_flush_log_at_trx_commit] 参数 是 commiit 操作时写 redolog 的行为 ，默认值配置的是 1；</p>\n<ol>\n<li>属性值 为 0 时，事务提交时，不会对重做日志进行写入操作，而是等待主线程按时写入，每秒写入一次，</li>\n<li>当属性值为 1 时，事务提交时，会将重做日志写入文件系统缓存，并且调用文件系统的 fsync，将文件系统缓冲区的数据真正写入磁盘存储，确保不会出现数据丢失；（fsync 是操作系统的函数）</li>\n<li>当属性值为 2 时，事务提交时，也会将日志文件写入文件系统缓存，但是不会调用文件系统的 fsync , 而是让文件系统直接去判断何时将缓存写入磁盘。</li>\n</ol>\n<h4 id=\"2-buffer-poll-缓存池\"><a class=\"markdownIt-Anchor\" href=\"#2-buffer-poll-缓存池\">#</a> 2、Buffer poll 缓存池</h4>\n<p>包含内容，数据页 (16k)，索引页，自适应 hash 索引，双写缓冲区，修改缓冲区（插入缓冲区）</p>\n<ol>\n<li>数据页：当对数据进行修改时，先把对应的数据页（16k) 放到内存中，然后再进行修改操作，此时内存数据页和磁盘数据页不一致，此时内存的数据页和磁盘的数据页不一致，内存的数据页就形成了脏页，一旦事务提交，就会记录到 redolog，记录了对数据的修改，保证数据安全。当查询数据时，以内存数据为准。</li>\n<li>索引页：一旦向表中插入数据或者修改数据时，把索引页放到内存中，在内存中进行修改，数据安全同样是由 redolog 保证的。</li>\n<li>自适应 hash 索引：Innodb 会根据分为频率和模式，为热点页建立哈希索引，来提高查询效率。不用我们自己维护。</li>\n<li>双写缓冲区：内存数据落盘时，需要使用双写缓冲区。</li>\n<li>修改缓冲区：也叫插入缓冲区，主要是对辅助索引修改时做的一个缓冲，辅助索引就是非主键索引。在早期版本中主要是针对插入操作。在新版本中，修改和删除页进行了缓冲</li>\n</ol>\n<h4 id=\"3-内存数据落盘重要\"><a class=\"markdownIt-Anchor\" href=\"#3-内存数据落盘重要\">#</a> 3、内存数据落盘（重要）</h4>\n<p>当要做增删改操作时，会对 Buffer pool 操作时， 先写 rdo log 到缓冲，然后进行落盘操作，<br>\ncheckPoint 触发时机:<br>\n 假设如果重做日志可以无限增大，同时缓冲池页足够大，那么是不需要将缓冲池页的新版本刷回磁盘，因为当发生宕机时，完全可以通过重做日志来恢复整个数据库系统中的数据到宕机发生时刻。需要两个前提， 缓冲池中可以缓存数据库中的所有数据，重做日志可以无限增大。</p>\n<p>因此就有了 checkPoint，主要解决了 1、缩短数据库恢复时间，2、缓冲池不够用时，将脏页刷新到磁盘，3. 重做日志不可用时刷新脏页<br>\n checkPonint 分类：可分为 sharp checkPonint (强制) 和 fuzzy checkPonint (模糊) ：<br>\nsharp checkPoint ：仅在关闭数据库的时候，将 BufferPool 中的脏页全部刷新到磁盘中，<br>\nfuzzy checkPoint : 数据库正常运行时，在不同的时机，将部分脏页写入磁盘，仅刷新部分脏页到磁盘。避免一次性刷新全部脏页造成性能问题。<br>\n有四个时机：<br>\n1、Master Thread CheckPoint : 主线程定时将脏页写入磁盘，每秒将脏页写入，定时落盘<br>\n 2、FLUSH_LRU_LIST CheckPoint 当 buffer pool 中脏页需要被淘汰时触发 checkpoint。<br>\n3、Async/sync Flush CheckPoint ; 异步或者同步落盘操作。跟 redolog 相关的，<br>\n4、Dirty Page too much CheckPoint ：buffer pool 中脏页过多时落盘，这个指标可配置</p>\n<h4 id=\"4-脏页落盘过程\"><a class=\"markdownIt-Anchor\" href=\"#4-脏页落盘过程\">#</a> 4、脏页落盘过程</h4>\n<p>双写机制：<br>\n1、先把脏页写到 double write buffer (内存双写缓冲区)<br>\n 2、把 double write buffer 的数据先写到系统表空间，<br>\n3、把缓冲区的数据写入用户表空间中；<br>\n4、如果写入系统表空间发生意外导致失败，可以使用用户表空间的数据页 + redolog 恢复数据。<br>\n5、如果系统表空间写入成功，用户表空间写入失败，可以使用系统表空间备份的数据页来恢复用户表空间的数据页。<br>\n使用双写机制，保证数据落盘过程万无一失。防止再写的过程中断电，造成数据丢失。</p>\n<h1 id=\"七-mysql性能优化篇\"><a class=\"markdownIt-Anchor\" href=\"#七-mysql性能优化篇\">#</a> 七、MYSQl 性能优化篇</h1>\n<ol>\n<li>首先需要使用慢查询日志，去获取查询时间比较长的 SQL 语句</li>\n<li>查看执行计划，查看有问题的 SQL 执行计划</li>\n<li>针对查询慢的 SQL 语句进行优化</li>\n<li>使用 SHOW profile 查看有问题的 SQL 性能使用情况</li>\n<li>调整操作系参数优化</li>\n<li>升级服务器硬件</li>\n</ol>\n<p>需要手动开启慢查询日志，需要设置阈值。</p>\n<p>#临时开启慢查询日志命令<br>\n set global slow_query_log=ON<br>\nset global long_query_time=1<br>\n# 数据库重启失效，长期开启需要去配置文件启动</p>\n<h4 id=\"分析慢查询日志的工具\"><a class=\"markdownIt-Anchor\" href=\"#分析慢查询日志的工具\">#</a> 分析慢查询日志的工具</h4>\n<p>explain 命令 查看执行计划</p>\n<h4 id=\"查看执行执行\"><a class=\"markdownIt-Anchor\" href=\"#查看执行执行\">#</a> 查看执行执行</h4>\n<p>使用 explain 命令 可以对 SQL 语句的执行计划进行分析</p>\n<p>id: SELECT 查询的标识符。每个 SELECT 都会自动分配一个唯一的标识符.<br>\nselect_type: SELECT 查询的类型.<br>\ntable: 查询的是哪个表<br>\n partitions: 匹配的分区<br>\n type: join 类型 查询类型<br>\n possible_keys: 此次查询中可能选用的索引<br>\n key: 此次查询中确切使用到的索引.<br>\nref: 哪个字段或常数与 key 一起被使用<br>\n rows: 显示此查询一共扫描了多少行。这个是一个估计值.<br>\nfiltered: 表示此查询条件所过滤的数据的百分比<br>\n extra: 额外的信息</p>\n<h4 id=\"sql语句的优化\"><a class=\"markdownIt-Anchor\" href=\"#sql语句的优化\">#</a> SQL 语句的优化</h4>\n<h5 id=\"1-索引的优化\"><a class=\"markdownIt-Anchor\" href=\"#1-索引的优化\">#</a> 1、索引的优化</h5>\n<p>为搜索字段、排序字段、select 查询列建立合适的索引。<br>\n尽量建立组合索引，并注意顺序<br>\n尽量使用覆盖索引<br>\n索引长度尽可能短，短索引可以节省空间<br>\n索引的更新不能频繁，更新频繁的数据不适合建索引</p>\n<h5 id=\"2-limit优化\"><a class=\"markdownIt-Anchor\" href=\"#2-limit优化\">#</a> 2、limit 优化</h5>\n<p>如果预计查询结果只有一个，建议使用 limit 1 可以停止全表扫描<br>\n处理分页会使用到 limit 当翻页到后面的时候偏移量会非常大， limit offset size<br>\nlimit 优化问题，其实是 offset 的问题，他会导致 mysql 扫描大量不需要的行再抛弃掉。<br>\n3、其他查询优化<br>\n小表驱动大表，建议使用 left join 时，以小表关联大表，<br>\n避免全表扫描<br>\n where 条件中尽量使用 Not in 语句建议使用 not exists<br>\n 合理利用慢查询日志，explain 执行查询计划，show profile 查看 sql 执行时的资源使用情况</p>\n<p>4、服务器端优化<br>\n缓冲区优化，设置足够大的 innodb_buffer_pool_size 将数据读取到内存中，<br>\n降低磁盘写入次数，对于生产环境来说，部分日志是可以不用的，<br>\nmysql 数据库配置优化 设置缓冲池内存大小<br>\n日志落盘配置</p>\n<p>5、操作系统优化<br>\n内核参数优化</p>\n<p>6、服务器硬件优化<br>\n提升硬件设备，例如选择尽量高频率的内存（频率不能高于主板的支持）、提升网络带宽、使用 SSD 高<br>\n速磁盘、提升 CPU 性能等。</p>\n<h1 id=\"七-mysql集群篇\"><a class=\"markdownIt-Anchor\" href=\"#七-mysql集群篇\">#</a> 七、MySQL 集群篇</h1>\n<h2 id=\"主从复制\"><a class=\"markdownIt-Anchor\" href=\"#主从复制\">#</a> 主从复制</h2>\n<p>2、binlog 和 relay 日志<br>\n bin log 记录了所有数据的更改，用于本机数据恢复和主从同步<br>\n relayz log ：中继日志</p>\n<ol>\n<li>mysql 主节点将 binlog 写入本地，从节点定时请求增量 binlog，主节点将 binlog 同步到从节点</li>\n<li>从节点单独进程会将 binlog 拷贝至本地 relaylog 中</li>\n<li>从节点定时重放 relay log。</li>\n</ol>\n<p>binlog 的日志模式：<br>\n1.statement level 模式（日志有小概率无法同步的）<br>\n2.rowlevel 模式 (比较耗存储性能)<br>\n 3.mixed 模式（实际上就是前两种模式的结合，在 mixed 模式下）<br>\n在配置文件中开启开启 binlog</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#binlog刷盘策略</span><br><span class=\"line\">sync_binlog=1</span><br><span class=\"line\"></span><br><span class=\"line\">0 ：存储引擎不进行binlog的刷新到磁盘，而由操作系统的文件系统控制缓存刷新。</span><br><span class=\"line\">1：每提交一次事务，存储引擎调用文件系统的sync操作进行一次缓存的刷新，这种方式最安全，但性</span><br><span class=\"line\">能较低。</span><br><span class=\"line\">n：当提交的日志组=n时，存储引擎调用文件系统的sync操作进行一次缓存的刷新。</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#需要备份的数据库</span><br><span class=\"line\">binlog-do-db=hello</span><br><span class=\"line\">#不需要备份的数据库</span><br><span class=\"line\">binlog-ignore-db=mysql</span><br><span class=\"line\">#启动二进制文件</span><br><span class=\"line\">log-bin=mysql-bin</span><br><span class=\"line\">#服务器ID</span><br><span class=\"line\">server-id=132</span><br></pre></td></tr></table></figure>\n<p>调整 binlog 日志模式<br>\n binlog 的三种格式： STATEMENT 、 ROW 、 MIXED 。</p>\n<p>查看 bin log 和 relay log 日志<br>\n因为 binlog 日志文件：mysql-bin.000005 是二进制文件，没法用 vi 等打开，这时就需要 mysql 的自带的<br>\n mysqlbinlog 工具进行解码，执行： mysqlbinlog mysql-bin.000005 可以将二进制文件转为可阅读的<br>\n sql 语句。</p>\n<p>3、基于 binlog 主从复制<br>\n关闭主从机器的防火墙</p>\n<p>主从复制存在一定延迟问题，并且只保证主机对外提供服务，只是在后台为主机进行备份。<br>\n配置流程<br>\n主服务器配置：</p>\n<ol>\n<li>先做数据同步将主服务器的数据全量复制到从服务器</li>\n<li>主服务器上配置开启 binlog</li>\n<li>在主服务器上对复制数据的用户做授权操作</li>\n<li>使用 show master status 语句查看主服务器状态<br>\n从服务器配置：</li>\n<li>配置从服务器的 server-id 参数，在 My.cnf 文件中配置</li>\n<li>如果从服务器使用的是虚拟机，并且是通过克隆得到的虚拟机，需要删除 auto.cnf 文件，重启服务器，会重新生成此文件</li>\n<li>重启 mysql 服务</li>\n<li>change master to 命令</li>\n</ol>\n<h2 id=\"读写分离\"><a class=\"markdownIt-Anchor\" href=\"#读写分离\">#</a> 读写分离</h2>\n<p>学习书籍：</p>\n<p>《从根上理解 mysql》</p>\n<p>《小林 Coding》</p>\n",
            "tags": [
                "原理",
                "学习笔记"
            ]
        }
    ]
}